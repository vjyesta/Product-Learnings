Significance Level

It is the measure of strength of evidence that must be present in your sample to reject null hypothesis and conclude that the effect is statistically significant.(alpha)

It helps you make decisions about whether the differences observed between two groups (the A group and the B group) are statistically significant or if they could have occurred by random chance.

During the A/B test, you collect data and perform statistical analysis to calculate a p-value

Here's a step-by-step explanation:

Collect Data: Start by collecting data from your experiment, which involves comparing two groups (e.g., Group A and Group B) to see if there's a meaningful difference between them. This could be anything from testing a new website design to comparing the effectiveness of two different medications.

Formulate a Hypothesis: Next, you create two hypotheses:

Null Hypothesis (H0): This hypothesis assumes that there is no real difference or effect between the groups. Any observed differences are due to random chance or noise.
Alternative Hypothesis (Ha): This hypothesis suggests that there is a real difference or effect between the groups.
Calculate a Test Statistic: Using statistical methods (such as t-tests or chi-squared tests, depending on your data and experiment), you calculate a test statistic from your data. This test statistic helps you measure the difference between the groups.

Find the Probability: You then find the probability of getting a test statistic as extreme as the one you calculated, assuming that the null hypothesis (no real difference) is true. This probability is the p-value.

Compare to a Significance Level: You compare the p-value to a pre-set significance level (commonly 0.05 or 5%). If the p-value is less than or equal to this significance level, you may conclude that the observed difference is statistically significant. In other words, you have evidence to reject the null hypothesis and accept the alternative hypothesis.

In simpler terms, the p-value tells you how likely it is that the observed differences in your experiment occurred purely by random chance. If the p-value is very small (typically less than 0.05), it suggests that the observed differences are unlikely to be random, and there's evidence of a real effect or difference between the groups.


A Type 1 error, often denoted as α (alpha), is a statistical error that occurs when you incorrectly reject a null hypothesis that is actually true. In other words, it's a false positive error. This error implies that you have concluded there is a significant effect or difference when, in reality, there isn't one. Type 1 errors are associated with the risk of making a wrong decision when conducting hypothesis tests or statistical analyses.

Here's a simple example to illustrate Type 1 errors:

Imagine you're conducting a medical test to determine if a patient has a particular disease. The null hypothesis in this case could be that the patient is healthy (no disease). The alternative hypothesis would be that the patient has the disease.

Correct Outcome (No Type 1 Error): If you correctly conclude that the patient is healthy (rejecting the disease hypothesis) when they are indeed healthy, this is a correct outcome with no Type 1 error.

Type 1 Error (False Positive): However, if you incorrectly conclude that the patient has the disease (rejecting the null hypothesis) when they are actually healthy, this is a Type 1 error. It means you've made a false positive diagnosis, potentially causing unnecessary stress and treatment for the patient.

Type 1 errors are typically controlled by choosing a significance level (α) before conducting a statistical test. This significance level represents the maximum acceptable probability of making a Type 1 error. Commonly, a significance level of 0.05 (5%) is used, meaning that you are willing to accept a 5% chance of making a Type 1 error. Lowering the significance level reduces the risk of Type 1 errors but increases the risk of Type 2 errors (failing to detect a real effect).

In summary, a Type 1 error occurs when you wrongly conclude that there is an effect or difference when there isn't one, and it is controlled by choosing an appropriate significance level in hypothesis testing.


Alpha range 

High risk - 0.01 
Mid Risk - 0.05 
Low risk - 0.10 (bug fix)

For Netflix Recommendation - Significance level - 0.05





